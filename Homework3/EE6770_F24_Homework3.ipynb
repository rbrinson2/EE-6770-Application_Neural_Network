{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57dbcd9",
   "metadata": {},
   "source": [
    "# <center>EE 6770 Fall 2024: Homework 3</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c26514-d4cd-4130-816d-df98662ba291",
   "metadata": {},
   "source": [
    "1. __Write comments__ in the code to explain your thoughts.\n",
    "2. __Important: Execute the codes and show the results__. \n",
    "3. __Do your own work.__ \n",
    "\n",
    "### Submission:\n",
    " * __Submit this notebook file and the pdf version__ - remember to add your name in the filename.\n",
    " * Deadline: 11:59 pm, 9/23 (Monday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53d7aa-df48-4f30-a63d-12b2604c9139",
   "metadata": {},
   "source": [
    "## Assignment Objectives:\n",
    "\n",
    "In this assignment, you will implement a multi-layer neural network (with __at least two hidden layers__) to classify a spiral dataset. Building on the NumPy-based gradient descent algorithm we discussed in class, your goal is to train the neural network to meet the following performance benchmarks:\n",
    "\n",
    "* __Achieve a prediction accuracy of at least 95%.__\n",
    "* __Achieve a final loss of less than 0.01.__\n",
    "  \n",
    "This task expands on the insights you gained in Homework 2 from the __TensorFlow Playground__, where you experimented with various network architectures and hyperparameters for the spiral dataset classification. Apply your observations and follow the step-by-step instructions to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a100dc0",
   "metadata": {},
   "source": [
    "### Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a886d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281c7b6-718d-48cc-8d8d-500ea4e55fec",
   "metadata": {},
   "source": [
    "## <font color=blue>Section 1: Generate the Spiral Dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc95175-7a9a-4abc-8357-208eabfd7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral_data(n_points, noise=0.1):\n",
    "    \n",
    "    n = np.sqrt(np.random.rand(n_points, 1)) * 720 * (2*np.pi) / 360\n",
    "    d1x = -np.cos(n) * n + np.random.rand(n_points, 1) * noise\n",
    "    d1y = np.sin(n) * n + np.random.rand(n_points, 1) * noise\n",
    "    \n",
    "    return np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))), np.hstack((np.zeros(n_points), np.ones(n_points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6116bd3-e791-4ee5-8363-204d30921222",
   "metadata": {},
   "source": [
    "### Write your code to generate 250 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2be428-0961-4767-92ee-fc42abccd3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notation:\n",
    "# X = input data,  Y = labelled target (0 or 1)\n",
    "\n",
    "##  WRITE YOUR CODE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0538f73-8aa8-41e7-af39-36978c20af4f",
   "metadata": {},
   "source": [
    "### Plot the spiral data. The scrip is already written, so you only need to execute it and show the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd28404-2450-4d73-b338-b84348eef85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataset with custom colors: Dodger Blue for class 0, Orange for class 1\n",
    "\n",
    "plt.scatter(X[Y == 0, 0], X[Y == 0, 1], color='dodgerblue', label='Class 0', s=20)\n",
    "plt.scatter(X[Y == 1, 0], X[Y == 1, 1], color='orange', label='Class 1', s=20)\n",
    "\n",
    "plt.title('Spiral Dataset')\n",
    "plt.legend()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67aa8c9-e110-4aa2-bd24-0caae205b152",
   "metadata": {},
   "source": [
    "### Normalize the two input features with respect to the mean and standard deviation.\n",
    "### This is called `z-score normalization`, which helps to accelerate learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf049c-552d-47a9-bcae-f15bda52fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(X):\n",
    "\n",
    "    mu = np.mean(X, axis=0)        # find the mean of each column/feature          \n",
    "                                    \n",
    "    sigma  = np.std(X, axis=0)     # find the standard deviation of each column/feature            \n",
    "\n",
    "    X = (X - mu) / sigma           # Normalize each column wrt its mean and std\n",
    "\n",
    "    return X, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c29181-0386-4129-8205-c10568d57daf",
   "metadata": {},
   "source": [
    "### Write codes to normalize the spiral dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb4f2c2-829b-45f1-b402-953726904efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0dee2",
   "metadata": {},
   "source": [
    "### Activation Function Library\n",
    "#### Those functions will be handy in calculating the forward and back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(z):\n",
    "    '''\n",
    "    derivative of sigmoid function\n",
    "    '''\n",
    "    a = sigmoid(z)\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3925b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drelu(z):\n",
    "    '''\n",
    "    derivative of ReLU function\n",
    "    '''\n",
    "    return np.where(z < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1db229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtanh(z):\n",
    "    '''\n",
    "    derivative of tanh function\n",
    "    '''\n",
    "    a = np.tanh(z)\n",
    "    return 1 - a**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3492bf7-f7ee-4054-b116-2da0cf7085be",
   "metadata": {},
   "source": [
    "## <font color=blue>Section 2: Set Up Neural Network Architecture</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85458806",
   "metadata": {},
   "source": [
    "### Neural Network Parameters\n",
    "\n",
    "Here is where you specify the NN architecture of your choice, which shall include:\n",
    "* Number of hidden layers\n",
    "* Number of neurons in each of the hidden layer\n",
    "* Number of neurons in the output layer (please set it as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b9f03-b5ef-41d2-92f7-66d26bbb07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[1]   # number of input features\n",
    "m = X.shape[0]   # number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ed3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE\n",
    "\n",
    "           # number of neurons in the hidden layer 1\n",
    "           # number of neurons in the hidden layer 2\n",
    "           # .....\n",
    "           # number of neurons in the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2b520",
   "metadata": {},
   "source": [
    "## <font color=blue>Section 3: Run Gradient Descent to Optimize Weights</font>\n",
    "\n",
    "In this section, you will use the `NumPy implementation` we covered in class to perform `forward propagation` and `backpropagation`, allowing you to compute the gradients and apply the gradient descent algorithm to find the optimal weights for your neural network.\n",
    "\n",
    "__Note: Use the Sigmoid activation function for the output layer. For the hidden layers, you may choose the activation function that best fits your network design.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c34e40",
   "metadata": {},
   "source": [
    "### Set up hyperparameters\n",
    "\n",
    "- __<font color='green'> Learning Rate</font>__: Experiment with different values of the learning rate. Start with a small value (e.g., 0.05) and gradually increase or decrease.\n",
    "- __<font color='green'> Number of Iterations</font>__: Vary the number of iterations (epochs) for training. Begin with a moderate number (e.g., 10000) and adjust accordingly. \n",
    "- __<font color='green'> Random Initialization</font>__: Avoid initializing weights to zero. Randomize the initial values to prevent symmetries during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE\n",
    "\n",
    "alpha = ??     # learning rate\n",
    "epoch = ??\n",
    "\n",
    "## WRITE YOUR CODE. Random initialization of weights and biases\n",
    "\n",
    "W1 = ??\n",
    "b1 = ??\n",
    "W2 = ??\n",
    "b2 = ??\n",
    "W3 = ??\n",
    "b3 = ??\n",
    ".....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5c71d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c284e4ff",
   "metadata": {},
   "source": [
    "### Run Gradient Descent\n",
    "\n",
    "For each epoch, we will execute the following:\n",
    "\n",
    "* __Forward propagation__\n",
    "* __Calculate the cost function, $J(W,b)$, and store it in an array__\n",
    "* __Backward propagation__\n",
    "* __Update the weights & biases for each layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00378f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "J = []      # An empty list for storing the loss function in each epoch\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    # Forward progagation. Write your code.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate and save the loss. Write your code.\n",
    "    \n",
    "    \n",
    "    # Back progagation. Write your code.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Update weights and biases. Write your code\n",
    "    \n",
    "    \n",
    "    # Display loss function periodically\n",
    "    \n",
    "    if i% math.ceil(epoch/20) == 0:\n",
    "        print(f\"{i:9d} {J[-1]:0.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e7241",
   "metadata": {},
   "source": [
    "### Print the following outputs:\n",
    "* #### Final loss function, $J$. <font color='red'>Is it less than 0.01?</font> \n",
    "* #### The first 10 elements of $A^{[L]}$ (the activation output at the Output Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE to print the final loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE to print the first 10 elements of A^L.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e8583",
   "metadata": {},
   "source": [
    "### Print the optimum weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ffc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODES\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b906c",
   "metadata": {},
   "source": [
    "### What are the predicted outputs, $\\hat{Y}$?\n",
    "#### Recall that, for binary classification, $\\hat{y} = 1$, if $a^{[L]} \\ge 0.5$ and $\\hat{y} = 0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e32be-e11e-48be-9e99-52bde6ad1532",
   "metadata": {},
   "source": [
    "## <font color=blue>Section 4: Performance Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83d813",
   "metadata": {},
   "source": [
    "#### <font color='green'>Prediction Accuracy</font> is obtained by comparing the labelled target, $Y$, with the predicted output, $\\hat{Y}$. It is defined as $\\frac{\\text{Total Number of Errors}}{\\text{Total Number of Samples}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296a50d-12ec-4f76-a0a2-7df2a531e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938c6bd",
   "metadata": {},
   "source": [
    "### Execute the following script to plot the Loss Function vs. Epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "plt.plot(J, color='orange')\n",
    "\n",
    "plt.title('Loss Function vs. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Function')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169ee8c-2db7-4da5-9237-5cb314532e1c",
   "metadata": {},
   "source": [
    "## <font color=blue>Extra Credit: visualize the decision boundary</font>\n",
    "* The following script plots the decision boundary using the given weights and biases. It is designed for a 3-layer neural network, but you can easily modify the code to suit your own network architecture.\n",
    "* You will need to implement the `forward_propagation` function based on your network's structure.\n",
    "* It’s also a great learning opportunity to generate this plot at different stages of training (e.g., after 1000 epochs, 5000 epochs, and 10,000 epochs) to observe how the decision boundary evolves as the model learns and improves its classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a meshgrid of points\n",
    "def create_meshgrid(X, h=0.02):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "# Forward propagation through the network\n",
    "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
    "\n",
    "    ## WRITE YOUR CODE\n",
    "    \n",
    "\n",
    "# Predict function that computes output for each point in the meshgrid\n",
    "def predict_on_grid(xx, yy, W1, b1, W2, b2, W3, b3):\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]  # Combine the grid points into a shape for input\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_propagation(grid_points, W1, b1, W2, b2, W3, b3)\n",
    "    predictions = (A3 >= 0.5).astype(int)  # Convert sigmoid output to binary class (0 or 1)\n",
    "    \n",
    "    return predictions.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "def plot_decision_boundary(X, y, W1, b1, W2, b2, W3, b3):\n",
    "    xx, yy = create_meshgrid(X)\n",
    "    Z = predict_on_grid(xx, yy, W1, b1, W2, b2, W3, b3)\n",
    "    \n",
    "    # Plot the contour map for decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    \n",
    "    # Plot the original data points\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c='dodgerblue', label='Class 0', s=12)\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='orange', label='Class 1', s=12)\n",
    "    plt.legend()\n",
    "    plt.title('Decision Boundary Visualization')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after training your neural network\n",
    "plot_decision_boundary(X, Y, W1, b1, W2, b2, W3, b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bcd28-94b5-472a-8342-6570a207abb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
